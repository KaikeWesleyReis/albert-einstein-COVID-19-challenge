{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19: A predictive analysis\nby: Kaike W. Reis\n\n\n## Steps\n- Missing Data Analysis & Pre-processing\n- Exploratory Data Analysis\n- Predictive Analysis - General Information\n- Task 1\n- Task 2\n- Conclusions\n\n## Notebook Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standard modules\nimport numpy as np\nimport pandas as pd\n\n# Machine Learning modules - quantitative analysis\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Graphical modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Display module\nfrom IPython.display import display","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Data Analysis & Pre-processing\n\n## Importing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing raw dataset\ndataset_raw = pd.read_excel('/kaggle/input/covid19/dataset.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unused columns\ndataset_raw.drop('Patient ID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change column names: lowercase and '_' over space\nnew_columns = list()\n# Loop over all columns\nfor col in dataset_raw.columns:\n    new_columns.append(col.lower().replace(' ','_'))\n# Modify dataset columns\ndataset_raw.columns = new_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data Analysis\n\n### **How many NaN values have each column?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many nan have each column\nnan_per_column = pd.DataFrame(dataset_raw.isna().sum(),columns=['nanValues']).reset_index()\n\n# Calculate NaN %\nfor i in range(0,len(nan_per_column)):\n    nan_per_column.loc[i, 'nanValuesPct'] = 100*round(nan_per_column.loc[i, 'nanValues']/len(dataset_raw),3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot - % of missing rows for each column\nplt.figure(figsize=(20,15))\nsns.barplot(x=\"index\", y=\"nanValuesPct\", data=nan_per_column)\nplt.xlabel('Variables', fontsize=20)\nplt.ylabel('Missing %', fontsize=20)\nplt.title('Missing Data Plot', fontsize=30)\nplt.yticks([0,10,20,30,40,50,60,70,80,90,100])\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this plot, It's clear that this dataset have a lot of missing values, so before models development it's necessary to have a complete dataset to train. \n\nFirst I have to decide what happens with our NaN values:\n- Get only complete samples?\n- Impute all missing values?\n\nWell, both solutions are not an option. The first one probably will select a complete dataset with 0 samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dataset_raw.dropna(how='any'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second one: \"Impute all missing values\" can be complicated. In Missing Data Analysis (field that studies forms to impute data) there are some golden rules (more like recommendations):\n- You need to understand if your data is Missing Completely at Random (MCAR), Missing at Random (MAR) or missing Not At Random (MNAR)\n- Samples with more than 50% missing data should not be imputed\n- Columns with more than 5%-10% should not be imputed\n\nLet's see an example:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boolean dataset for missing values, except sars-cov-2\ndataset_nan = dataset_raw.drop('sars-cov-2_exam_result',axis=1).isnull().join(dataset_raw['sars-cov-2_exam_result'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20,15))\nfeatures_to_plot = dataset_nan.columns[4:24] # Avoid first 4 variables because they are complete and are target variables\nr = 0 # Index row\nc = 0 # Index col\nfor f in features_to_plot:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=dataset_nan,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 3:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20,15))\nfeatures_to_plot = dataset_nan.columns[24:44] # Avoid first 4 variables because they are complete and are target variables\nr = 0 # Index row\nc = 0 # Index col\nfor f in features_to_plot:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=dataset_nan,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 3:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20,15))\nfeatures_to_plot = dataset_nan.columns[44:64] # Avoid first 4 variables because they are complete and are target variables\nr = 0 # Index row\nc = 0 # Index col\nfor f in features_to_plot:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=dataset_nan,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 3:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20,15))\nfeatures_to_plot = dataset_nan.columns[64:84] # Avoid first 4 variables because they are complete and are target variables\nr = 0 # Index row\nc = 0 # Index col\nfor f in features_to_plot:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=dataset_nan,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 3:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20,15))\nfeatures_to_plot = dataset_nan.columns[84:104] # Avoid first 4 variables because they are complete and are target variables\nr = 0 # Index row\nc = 0 # Index col\nfor f in features_to_plot:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=dataset_nan,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 3:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20,15))\nfeatures_to_plot = dataset_nan.columns[104:109] # Avoid first 4 variables because they are complete and are target variables\nr = 0 # Index row\nc = 0 # Index col\nfor f in features_to_plot:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=dataset_nan,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 1:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From those graphs you can see:\n- Doing Average imputation you may be generalizing negative cases into positive cases, which can be considered an error. Even because, how can you extrapolate the values of a positive case only with values for negative cases?\n- It is possible to verify that there is a column with at all missing values, that is, a Zero imputation would add a variable without deviation and therefore useless to the model.\n- Even through a Selected Case Average imputation (imputing negative cases with negative cases average and positive cases with positive cases average) can cause two problems such as: lack of a considerable samples (mainly for positive cases) to propose a representative average  or situations where positive cases do not even present samples to take the average.\n\nBesides that Mean imputation or Zero imputation are not recommended given the diversity of imputation techniques.\n\nSo it's important to present a consistent dataset before any model, to garantee a representative prediction in the future. To continue my analysis I will develop a function to find a complete dataset respecting the ideas:\n- Keep the higher number of variables\n- Keep most of the rows\n\nGiven the fact that to eval all 109! dataset possibilities combinations (1.4438595832025E+176 combinations), I will propose a different approach...\n\n### Function to find a local complete dataset\nArguments:\n- ```rows_threshold``` - How many complete information each columns must have in respect to ```target_variable```\n- ```target_variable``` - Target variable to evaluate previous argument\n- ```df``` - Dataset to verify\n\nThe function routine is:\n- Evaluate how many values each column have with respect to ```target_variable``` to filter based in ```rows_threshold``` value\n- Sort this result in Ascending or Descending way\n- Print how many complete rows do we have for some amount of variables as we increase the variable number in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_complete_dataset(rows_threshold, target_variable, df, asc):\n    # Count how many existent values each variable have in relation to target_variable\n    df_value_counts = pd.DataFrame(df.groupby(target_variable).count())\n    # Create a df to keep the variables that contains: existent values >= threshold\n    df_vars_threshold = pd.DataFrame(columns=['vars','values'])\n    # Evaluate the variables that respect this condition\n    for var in df_value_counts.columns:\n        # Sum samples from groupby.count\n        existent_samples = df_value_counts[var].sum()  \n        if existent_samples >= rows_threshold:\n            index = len(df_vars_threshold)\n            df_vars_threshold.loc[index, 'vars'] = var\n            df_vars_threshold.loc[index, 'values'] = existent_samples\n    # Sort Descending dataframe (Assuming that higher complete values cause a best choice to keep most of the variables)\n    df_vars_threshold.sort_values(by=['values'], ascending=asc, inplace=True)\n    # List all variables (features) that pass the established condition \n    vars_threshold = list(df_vars_threshold['vars'])\n    # Print Info\n    print('### For ',rows_threshold,' rows samples complete we have ',len(vars_threshold),' possible feature variables.')\n    # Verify\n    for i in range(1, len(vars_threshold)+1):\n        # Define the set of variables to verify\n        vars_to_test = vars_threshold[0:1+len(vars_threshold)-i]\n        df_model = df[vars_to_test].dropna(how='any')\n        # Print Info\n        print('With ', len(vars_to_test),' variables, we have ', len(df_model),' complete rows.')\n    # Return sorted variables\n    return vars_threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variables to keep out from feature dataset\ntargets_out = ['patient_addmited_to_semi-intensive_unit_(1=yes,_0=no)','patient_addmited_to_intensive_care_unit_(1=yes,_0=no)',\n               'patient_addmited_to_regular_ward_(1=yes,_0=no)']\n\n# Raw data for features (keep target sars)\ndata = dataset_raw.drop(targets_out, axis=1)\n\n# Eval a possible dataset given my assumption for Ascending sorting\ndataset_vars_eval_asc = find_complete_dataset(rows_threshold=500, target_variable='sars-cov-2_exam_result', df=data, asc=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval a possible dataset given my assumption for Descending sorting\ndataset_vars_eval_des = find_complete_dataset(rows_threshold=500, target_variable='sars-cov-2_exam_result', df=data, asc=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based in this approach, I found two possible datasets:\n- Ascending way with 420 samples\n- Descending way with 1352 samples\n\nLet's explore more the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating two complete datasets\n## Ascending\nvars_selected = ['sars-cov-2_exam_result'] + targets_out + dataset_vars_eval_asc[0:16]\ndf1 = dataset_raw[vars_selected].dropna(how='any')\ndf1.index = range(0,len(df1))\n## Show info\nprint('### Ascending way')\nfor i in df1.columns:\n    print(i)\n## Descending\nvars_selected = ['sars-cov-2_exam_result'] + targets_out + dataset_vars_eval_des[0:18]\ndf2 = dataset_raw[vars_selected].dropna(how='any')\ndf2.index = range(0,len(df2))\n## Show info\nprint('\\n### Descending way')\nfor i in df2.columns:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View df\ndisplay(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View df\ndisplay(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you noticed, are two distincting datasets:\n- Different variables\n- Different type (categorical df vs numerical df)\n\nThe EDA phase will decide which dataset will be use to model development."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n\n## EDA for Numerical complete local dataset\n\n**PS**: Data already have mean 0 and std as 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval 'sars-cov-2_exam_result' proportions\nprint('Positive case proportion - original dataset [%]: ', round(100*dataset_raw['sars-cov-2_exam_result'].value_counts()[1]/dataset_raw['sars-cov-2_exam_result'].value_counts().sum(),2))\nprint('Positive case proportion - numerical dataset [%]: ', round(100*df1['sars-cov-2_exam_result'].value_counts()[1]/df1['sars-cov-2_exam_result'].value_counts().sum(),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's possible to see that we gain more positive proportion in this complete dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval features for 'sars-cov-2_exam_result'\n## Defining our Y variables out\ntargets_out = ['sars-cov-2_exam_result','patient_addmited_to_semi-intensive_unit_(1=yes,_0=no)','patient_addmited_to_intensive_care_unit_(1=yes,_0=no)',\n           'patient_addmited_to_regular_ward_(1=yes,_0=no)']\n\n## Defining our X variables\nfeat_cols = list(set(df1.columns).difference(set(targets_out)))\n\n## Data\ndata = df1[['sars-cov-2_exam_result']+feat_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Plot NaN Bool Dataset related to sars\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20,15))\nr = 0 # Index row\nc = 0 # Index col\nfor f in data.columns:\n    if not f == 'sars-cov-2_exam_result':\n        sns.stripplot(x=f, y='sars-cov-2_exam_result', data=data, ax=axes[r][c])\n        #axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n        # Index control\n        c += 1\n        if c > 3:\n            c = 0\n            r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this result, I analyze that none of the variables could prove linearly exam_results (don't present boundary regions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, hue='sars-cov-2_exam_result')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through this amount of plots, it's possible to see that some variables are correlated. Let's evaluate this through a correlation heatmap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nmyBasicCorr = df1[feat_cols].corr('spearman')\nsns.heatmap(myBasicCorr, annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering a correlation absolute threshold of 0.90 (maximum is 1), two pairs needs to be evaluate:\n- neutrophils VS lymphocytes\n- hemoglobin VS hematocrit\n\nBecause they are higher correlated, to decrease model complexity and noise would be necessary to remove one of them for each pair. First let's evaluate each data distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dist plot\ncorr_vars = ['neutrophils', 'lymphocytes', 'hemoglobin', 'hematocrit']\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20,15))\nr = 0 # Index row\nc = 0 # Index col\n# Array for each category\ntarget_0 = data.loc[data['sars-cov-2_exam_result'] == 'negative']\ntarget_1 = data.loc[data['sars-cov-2_exam_result'] == 'positive']\n# Plot process\nfor f in corr_vars:\n        sns.distplot(target_0[f], hist=False, ax=axes[r][c])\n        sns.distplot(target_1[f], hist=False, ax=axes[r][c])\n        plt.legend(title='sars_case')\n        c += 1\n        if c > 1:\n            c = 0\n            r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the distribution, I will keep: ```hematocrit``` and ```lymphocytes``` most because the distinguished difference between positive/negative cases distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Atualize main df\ndf1.drop(['hemoglobin', 'neutrophils'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, for continuos dataset we have 16 features!"},{"metadata":{},"cell_type":"markdown","source":"## EDA for Categorical complete local dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval 'sars-cov-2_exam_result' proportions\nprint('Positive case proportion - original dataset [%]: ', round(100*dataset_raw['sars-cov-2_exam_result'].value_counts()[1]/dataset_raw['sars-cov-2_exam_result'].value_counts().sum(),2))\nprint('Positive case proportion - numerical dataset [%]: ', round(100*df2['sars-cov-2_exam_result'].value_counts()[1]/df2['sars-cov-2_exam_result'].value_counts().sum(),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Different from numerical dataset, was lost positive proportion having a more unbalanced dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval features for 'sars-cov-2_exam_result'\n## Defining our Y variables out\ntargets_out = ['sars-cov-2_exam_result','patient_addmited_to_semi-intensive_unit_(1=yes,_0=no)','patient_addmited_to_intensive_care_unit_(1=yes,_0=no)',\n           'patient_addmited_to_regular_ward_(1=yes,_0=no)']\n\n## Defining our X variables\nfeat_cols = list(set(df2.columns).difference(set(targets_out)))\n\n## Data\ndata = df2[['sars-cov-2_exam_result']+feat_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT - Barplots over our variables\nfig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20,15))\nr = 0 # Index row\nc = 0 # Index col\nfor f in feat_cols:\n    # Count Plot\n    sns.countplot(x=f, hue='sars-cov-2_exam_result', data=data,ax=axes[r][c])\n    # Plot configs\n    axes[r][c].legend(title='sars-cov-2_exam_result', loc='upper right')\n    # Index control\n    c += 1\n    if c > 3:\n        c = 0\n        r += 1\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the fact that most of the variables just present one category with positive samples and target variable proportion lost, **I will discontinue the study in discrete dataset**."},{"metadata":{},"cell_type":"markdown","source":"# Predictive Analysis - General Information\nFor both tasks, those infos are necessary to a better comprehension.\n\n## Why Support Vector Machine (SVM)\n- It's is a robust model, before deep learning explosion as one of the most used\n- Works really well with small datasets, as I only have 420 samples. This statement is not applied for Deep Learning models that requires a lot of data to work a correct generalization.\n- It's a powerful binary classifier and this problem is a binary classification (TASK 1). Nonetheless, can be applied for multi-class problem (TASK 2)\n\n## Measures againts imbalanced target class\nAll targets for task 1 and 2 are unbalanced, to avoid this I proposed:\n- Train/Test split stratified\n- Stratified K-Fold Cross-Validation, keeping the proportion between train/validation split\n- Cross Validation Analysis to avoid possible overfit due to small dataset\n- Score metrics - balanced accuracy\n- Evaluate confusion matrix results besides accuracy\n\n**PS**: Stratify keeps the same proportion for target categories in spliting process.\n\n## Steps for model development\n- Prepare the continuous dataset\n- Split a test set to verify the metrics\n- Grid Search Stratified KFold Cross Validation for SVM Classifier\n- Evaluate Metrics\n\n## Metrics details\nWill be evaluated **balanced accuracy** and a **confusion matrix report**. To understand the last one, let me explaing a binary confusion matrix:\n\n![cm](https://user-images.githubusercontent.com/32513366/77873202-aec63780-721f-11ea-9955-08e3860e2a01.PNG)\n\nColumn - Predicted value\nRow - Real value\n\nAs the target variable is medical in nature, each value in the confusion matrix has a meaning:\n- TP (True Positive): If a pacient present COVID-19 and the model predict correct\n- TN (True Negative): If a pacient don't present COVID-19 and the model predict correct\n- FP (False Positive): If a pacient don't present COVID-19 and the model says that he's infected\n- FN (False Negative): If a pacient present COVID-19, but the model says that he's fine\n\nGiven the actual world situation, I considered **FN error the worst**: the pacient after getting out from the hospital could infect others. I will considered **recall** and **accuracy** as the most important measures for this study."},{"metadata":{},"cell_type":"markdown","source":"# Task 1\n**Predict confirmed COVID-19 cases among suspected cases. Based on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive/negative)?**"},{"metadata":{},"cell_type":"markdown","source":"### Prepare the continuous dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Defining our X variables\nfeat_cols = list(set(df1.columns).difference(set(targets_out)))\n\n## Setting Binary values - One Hot Encode\ndf1.loc[df1['sars-cov-2_exam_result'] == 'positive', 'sars-cov-2_exam_result'] = 1\ndf1.loc[df1['sars-cov-2_exam_result'] == 'negative', 'sars-cov-2_exam_result'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split a test set to verify the metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df1[feat_cols], df1['sars-cov-2_exam_result'], test_size = 0.20, random_state = 1206, stratify=df1['sars-cov-2_exam_result'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search Stratified KFold Cross Validation for SVM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining parameter range to grid search\nparam_gridSVM = {'C': [0.1, 1, 10, 100, 1000],\n                 'shrinking':[True, False],\n                 'gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001, 0.0001], \n                 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \n\n# Define grid instance\ngridSVM = GridSearchCV(cv=5, estimator=SVC(class_weight='balanced', random_state=101), param_grid=param_gridSVM, refit = True, verbose = 0, scoring='balanced_accuracy') \n\n# Initialize grid search, fitting the best model\ngridSVM.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print best parameter after tuning \nprint(gridSVM.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print how our best model looks after hyper-parameter tuning \nprint(gridSVM.best_estimator_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Metrics\nEvaluating SVM model performance can answer the first task."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions over test set for both models\ny_pred_svm = gridSVM.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print classification report SVM\nprint(classification_report(y_test, y_pred_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix SVM\n## original binary labels\nlabels = np.unique(y_test)\n## DF with C.M.\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred_svm, labels=labels), index=labels, columns=labels)\n# Visualize labels\ncm.index = ['real: 0', 'real: 1']\ncm.columns = ['pred: 0', 'pred: 1']\n\n# CM visualization\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results - Task 1\n- The SVM model had a great performance, with a recall (related to **FN error**) of 75% and a  accuracy ~80%\n- The model is robust against unbalanced data for the task 1\n\nProbably with more complete samples for those continuous features the model can be improved!"},{"metadata":{},"cell_type":"markdown","source":"# Task 2\nPredict admission to general ward, semi-intensive unit or intensive care unit among confirmed COVID-19 cases. Based on the results of laboratory tests commonly collected among confirmed COVID-19 cases during a visit to the emergency room, would it be possible to predict which patients will need to be admitted to a general ward, semi-intensive unit or intensive care unit?\n\nTo evaluate this problem, I will create a categorical variable for all patient admitted.\n\n### Prepare the continuous dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Defining our X variables\nfeat_cols = list(set(df1.columns).difference(set(['sars-cov-2_exam_result'] + targets_out)))\n\n## Evaluate how many possibilities we have for three targets in a single column\npatient_addmited_possibilities = list()\nfor i in range(0, len(df1)):\n    possibility=str(df1.loc[i,targets_out[1]]) + str(df1.loc[i,targets_out[2]]) + str(df1.loc[i,targets_out[3]])\n    patient_addmited_possibilities.append(possibility)\n\n## Get unique possibilities\npatient_addmited_cats = sorted(set(patient_addmited_possibilities))\npatient_addmited_cats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output related to previous one\n[0,1,2,3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create a new dataset for this task with a new column\ndf1_t2 = df1.copy()\ndf1_t2['patient_addmited_cats'] = patient_addmited_possibilities\ndf1_t2.drop(targets_out, axis=1, inplace=True)\n\n## Change to num values\nfor i in range(0, len(df1_t2)):\n    if df1_t2.loc[i, 'patient_addmited_cats'] == patient_addmited_cats[0]:\n        df1_t2.loc[i, 'patient_addmited_cats'] = 0\n    elif df1_t2.loc[i, 'patient_addmited_cats'] == patient_addmited_cats[1]:\n        df1_t2.loc[i, 'patient_addmited_cats'] = 1\n    elif df1_t2.loc[i, 'patient_addmited_cats'] == patient_addmited_cats[2]:\n        df1_t2.loc[i, 'patient_addmited_cats'] = 2\n    elif df1_t2.loc[i, 'patient_addmited_cats'] == patient_addmited_cats[3]:\n        df1_t2.loc[i, 'patient_addmited_cats'] = 3\n## See df\ndf1_t2['patient_addmited_cats'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we can see a unbalanced problem."},{"metadata":{},"cell_type":"markdown","source":"### Split a test set to verify the metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df1_t2[feat_cols], df1_t2['patient_addmited_cats'], test_size = 0.20, random_state = 1206, stratify=df1_t2['patient_addmited_cats'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search Stratified KFold Cross Validation for SVM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining parameter range to grid search\nparam_gridSVM = {'C': [0.1, 1, 10, 100, 1000], \n                 'shrinking':[True, False],\n                 'gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001, 0.0001], \n                 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \n\n# Define grid instance\ngridSVM = GridSearchCV(cv=5, estimator=SVC(class_weight='balanced', random_state=101), param_grid=param_gridSVM, refit = True, verbose = 0, scoring='balanced_accuracy') \n\n# Initialize grid search, fitting the best model\ngridSVM.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print best parameter after tuning \nprint(gridSVM.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print how our best model looks after hyper-parameter tuning \nprint(gridSVM.best_estimator_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions over test set for both models\ny_pred_svm = gridSVM.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print classification report SVM\nprint(classification_report(y_test, y_pred_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix SVM\n## original binary labels\nlabels = np.unique(y_test)\n## DF with C.M.\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred_svm, labels=labels), index=labels, columns=labels)\n\n# CM visualization\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results - Task 2\n- The model provides a good **recall** of 67% for the first class (no need any hospitalization) and accuracy 58%.\n- Unfortunately it's verify that the model have worst prediction for others categories. This could be explain by several reasons, but based on my work here it's possible that the continuous feature space dataset it's not good enough for this task as was for the first one or given the multi-class classification problem would require more complete samples.\n\n\n# Conclusions\nAll pipeline was explained during code development. I think that is possible to keep those continuous variables and increase the size of complete samples to improve both models performance.\n\n#### Let's win this fight against Covid-19!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}